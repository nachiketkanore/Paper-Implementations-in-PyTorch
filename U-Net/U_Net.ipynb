{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-Net.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2De1dn6Z08W"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGE79pyzcMFN"
      },
      "source": [
        "def double_conv(in_channels, out_channels):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size = 3),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size = 3),\n",
        "        nn.ReLU(inplace = True)\n",
        "    )\n",
        "    return conv"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amBYmpHA9-7b"
      },
      "source": [
        "def crop_image(original, target):\n",
        "    target_size = target.size()[2]\n",
        "    original_size = original.size()[2]\n",
        "    delta = original_size - target_size\n",
        "    assert(delta >= 0)\n",
        "    delta = delta // 2\n",
        "    return original[:, :, delta: original_size-delta, delta:original_size-delta]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDEHV3sPbWjq"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.down_conv1 = double_conv(1, 64)\n",
        "        self.down_conv2 = double_conv(64, 128)\n",
        "        self.down_conv3 = double_conv(128, 256)\n",
        "        self.down_conv4 = double_conv(256, 512)\n",
        "        self.down_conv5 = double_conv(512, 1024)\n",
        "\n",
        "        self.trans_conv1 = nn.ConvTranspose2d(\n",
        "                in_channels = 1024,\n",
        "                out_channels = 512,\n",
        "                kernel_size = 2,\n",
        "                stride = 2\n",
        "            )\n",
        "    \n",
        "        self.up_conv1 = double_conv(1024, 512)\n",
        "\n",
        "        self.trans_conv2 = nn.ConvTranspose2d(\n",
        "                in_channels = 512,\n",
        "                out_channels = 256,\n",
        "                kernel_size = 2,\n",
        "                stride = 2\n",
        "            )\n",
        "    \n",
        "        self.up_conv2 = double_conv(512, 256)\n",
        "\n",
        "        self.trans_conv3 = nn.ConvTranspose2d(\n",
        "                in_channels = 256,\n",
        "                out_channels = 128,\n",
        "                kernel_size = 2,\n",
        "                stride = 2\n",
        "            )\n",
        "    \n",
        "        self.up_conv3 = double_conv(256, 128)\n",
        "\n",
        "        self.trans_conv4 = nn.ConvTranspose2d(\n",
        "                in_channels = 128,\n",
        "                out_channels = 64,\n",
        "                kernel_size = 2,\n",
        "                stride = 2\n",
        "            )\n",
        "    \n",
        "        self.up_conv4 = double_conv(128, 64)\n",
        "\n",
        "        self.out = nn.Conv2d(\n",
        "            in_channels = 64,\n",
        "            out_channels = 2,    # increase according to number of classes\n",
        "            kernel_size = 1\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        # image = [batch_size, channels, h, w]\n",
        "        # Encoder part\n",
        "        x1 = self.down_conv1(image) # use for concat\n",
        "        x2 = self.max_pool_2x2(x1)\n",
        "\n",
        "        x3 = self.down_conv2(x2) # use for concat\n",
        "        x4 = self.max_pool_2x2(x3)\n",
        "\n",
        "        x5 = self.down_conv3(x4) # use for concat\n",
        "        x6 = self.max_pool_2x2(x5)\n",
        "\n",
        "        x7 = self.down_conv4(x6) # use for concat\n",
        "        x8 = self.max_pool_2x2(x7)\n",
        "        \n",
        "        x9 = self.down_conv5(x8)\n",
        "\n",
        "        # image size after first part\n",
        "        print(f'Image after first part : {x9.shape}')\n",
        "\n",
        "        # Decoder part\n",
        "        x = self.trans_conv1(x9)\n",
        "        y = crop_image(x7, x)\n",
        "        x = self.up_conv1(torch.cat([x, y], 1))\n",
        "\n",
        "        x = self.trans_conv2(x)\n",
        "        y = crop_image(x5, x)\n",
        "        x = self.up_conv2(torch.cat([x, y], 1))\n",
        "\n",
        "        x = self.trans_conv3(x)\n",
        "        y = crop_image(x3, x)\n",
        "        x = self.up_conv3(torch.cat([x, y], 1))\n",
        "\n",
        "        x = self.trans_conv4(x)\n",
        "        y = crop_image(x1, x)\n",
        "        x = self.up_conv4(torch.cat([x, y], 1))\n",
        "\n",
        "        x = self.out(x)\n",
        "        print(x.size())\n",
        "        return x"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVBZyJs8dC-p",
        "outputId": "0de00d92-56c1-4aae-92c4-ecef6bb9b79d"
      },
      "source": [
        "sample = torch.randn(1, 1, 572, 572)\n",
        "print(sample.shape)\n",
        "model = UNet()\n",
        "print(model(sample)) # expected size according to paper = [1, 2, 388, 388]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 572, 572])\n",
            "Image after first part : torch.Size([1, 1024, 28, 28])\n",
            "torch.Size([1, 2, 388, 388])\n",
            "tensor([[[[ 0.0475,  0.0502,  0.0417,  ...,  0.0439,  0.0425,  0.0439],\n",
            "          [ 0.0486,  0.0437,  0.0477,  ...,  0.0289,  0.0350,  0.0436],\n",
            "          [ 0.0499,  0.0430,  0.0356,  ...,  0.0519,  0.0380,  0.0338],\n",
            "          ...,\n",
            "          [ 0.0382,  0.0460,  0.0478,  ...,  0.0526,  0.0222,  0.0196],\n",
            "          [ 0.0428,  0.0301,  0.0499,  ...,  0.0401,  0.0342,  0.0371],\n",
            "          [ 0.0316,  0.0365,  0.0528,  ...,  0.0428,  0.0544,  0.0505]],\n",
            "\n",
            "         [[-0.0805, -0.0946, -0.0886,  ..., -0.0820, -0.0899, -0.0962],\n",
            "          [-0.0995, -0.0774, -0.0865,  ..., -0.0999, -0.0961, -0.0926],\n",
            "          [-0.0864, -0.1052, -0.1003,  ..., -0.0984, -0.0968, -0.0962],\n",
            "          ...,\n",
            "          [-0.0918, -0.0828, -0.0936,  ..., -0.0887, -0.0881, -0.0937],\n",
            "          [-0.0861, -0.0995, -0.0926,  ..., -0.1038, -0.0891, -0.0805],\n",
            "          [-0.0954, -0.0854, -0.0886,  ..., -0.0825, -0.0784, -0.0967]]]],\n",
            "       grad_fn=<ThnnConv2DBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU9D2FT_fDRm"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    }
  ]
}